---
layout: post
title: "[DSLAB] Sequence-to-Sequence"

categories:
  - DSLAB
  - 3.Seq2seq&Attention

tags:
  - [NLP, RNN, Deep learning, Seq2seq, Attention, Encoder, Decoder]
---

`Sequence-to-Sequence` 모델은 앞서 나왔던 `RNN`의 구조 중 `Many-to-many`의 형태에 속한다. 입력이 Sequence, 출력도 Sequence인
형태에서 입력 Sequence를 모두 읽은 후 출력 Sequence를 예측하는 모델이다.

![이미지](https://epochai.org/assets/images/posts/2022/estimating-training-compute/rnn-many-to-many-2.png)

### Seq2seq의 구조

---

`Sequence-to-Sequence`의 구조는 입력된 문장을 읽어들이는 `Encoder`와 출력 문장을 단어 하나씩 생성하여 출력하는 `Decoder`로 이루어져
있다. 여기서 `Encoder`와 `Decoder`는 둘다 `RNN`모델이다.

![이미지](https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG)


