---
layout: post
title: "[DSLAB] Sequence-to-Sequence"

categories:
  - DSLAB
  - 3.Seq2seq&Attention

tags:
  - [NLP, RNN, Deep learning, Seq2seq, Attention, Encoder, Decoder]
---

`Sequence-to-Sequence` 모델은 앞서 나왔던 `RNN`의 구조 중 `Many-to-many`의 형태에 속한다. 입력이 Sequence, 출력도 Sequence인
형태에서 입력 Sequence를 모두 읽은 후 출력 Sequence를 예측하는 모델이다.

![이미지](https://epochai.org/assets/images/posts/2022/estimating-training-compute/rnn-many-to-many-2.png)

### Seq2seq의 구조

---

`Sequence-to-Sequence`의 구조는 입력된 문장을 읽어들이는 `Encoder`와 출력 문장을 단어 하나씩 생성하여 출력하는 `Decoder`로 이루어져
있다. 여기서 `Encoder`와 `Decoder`는 각각 다른 파라미터를 가지고있는 `RNN`모델이다.

![이미지](https://wikidocs.net/images/page/24996/%EC%9D%B8%EC%BD%94%EB%8D%94%EB%94%94%EC%BD%94%EB%8D%94%EB%AA%A8%EB%8D%B8.PNG)

`Encoder`의 마지막 time step에서의 `hidden state`벡터는 `Decoder`의 h0, 즉 첫번째 time step의 입력으로 주어지는 `hidden state`로써의
역할을 한다. 그리고 `Decoder`의 첫번째 time step 입력으로는 start토큰인 <SOS>토큰을 입력하게 되고 time step 출력에서 마지막 출력을 나타내는
<END>토큰이 나오게 된다면 최종적인 출력으로 인식해서 출력을 종료하는 과정을 따르게 된다.
